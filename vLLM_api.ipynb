{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "3YY9j9i5pO06",
        "2qMGSFSjo3vv",
        "yxE3MALX_qla",
        "sAOfXvmNqDK4",
        "voUfnk5UCn-Z",
        "MDbKQu3FC2BW",
        "xjG-oRrVD0Gw",
        "IaqhoQu-D4Yx",
        "xs9shOXPE6u9",
        "vPG4VGLMpvV2",
        "5FUM6x50pmTq",
        "1kvUp1Uk1v2u",
        "d6Vj81771kk5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpBWW_oAknIt"
      },
      "outputs": [],
      "source": [
        "!pip install vllm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vLLM API Endpoints - KapsamlÄ± Request/Response DokÃ¼manÄ±"
      ],
      "metadata": {
        "id": "3YY9j9i5pO06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Ä°Ã§indekiler\n",
        "\n",
        "1. [OpenAI Compatible Endpoints](#openai-compatible-endpoints)\n",
        "2. [Custom vLLM Endpoints](#custom-vllm-endpoints)\n",
        "3. [KullanÄ±m Ä°puÃ§larÄ±](#kullanim-ipuclari)\n",
        "\n",
        "---\n",
        "\n",
        "## OpenAI Compatible Endpoints\n",
        "\n",
        "### 1. Chat Completions API\n",
        "\n",
        "**Endpoint:** `POST /v1/chat/completions`\n",
        "\n",
        "**AÃ§Ä±klama:** Chat formatÄ±ndaki mesajlarÄ± iÅŸleyerek LLM'den yanÄ±t alÄ±r. OpenAI Chat API ile tam uyumludur. Api dÃ¶kÃ¼mantasyonu: https://platform.openai.com/docs/api-reference/chat/create\n",
        "\n",
        "#### Request Body\n",
        "\n",
        "| Field               | Type          | Required | AÃ§Ä±klama                                                  |\n",
        "| ------------------- | ------------- | -------- | --------------------------------------------------------- |\n",
        "| `model`             | string        | âœ“        | KullanÄ±lacak model adÄ± (Ã¶rn: \"openai/gpt-oss-20b\")        |\n",
        "| `messages`          | array         | âœ“        | KonuÅŸma mesajlarÄ± dizisi                                  |\n",
        "| `temperature`       | number        | âœ—        | Sampling temperature (0.0-2.0, default: 1.0)              |\n",
        "| `max_tokens`        | integer       | âœ—        | Maksimum Ã¼retilecek token sayÄ±sÄ±                          |\n",
        "| `top_p`             | number        | âœ—        | Nucleus sampling (0.0-1.0, default: 1.0)                  |\n",
        "| `frequency_penalty` | number        | âœ—        | Frequency penalty (-2.0 - 2.0, default: 0.0)              |\n",
        "| `presence_penalty`  | number        | âœ—        | Presence penalty (-2.0 - 2.0, default: 0.0)               |\n",
        "| `stop`              | string/array  | âœ—        | Stop sequence(s)                                          |\n",
        "| `stream`            | boolean       | âœ—        | Streaming response (default: false)                       |\n",
        "| `tools`             | array         | âœ—        | Function calling iÃ§in tool tanÄ±mlarÄ±                      |\n",
        "| `tool_choice`       | string/object | âœ—        | Tool seÃ§im stratejisi (\"auto\", \"none\", veya belirli tool) |\n",
        "| `extra_body`        | object        | âœ—        | vLLM Ã¶zel parametreleri                                   |\n",
        "\n",
        "#### Messages Object (Request)\n",
        "\n",
        "Ä°stek gÃ¶vdesindeki `messages` alanÄ±, aÅŸaÄŸÄ±daki rolleri iÃ§eren mesajlardan oluÅŸur:\n",
        "\n",
        "- `system`: Modelin davranÄ±ÅŸÄ±nÄ± tanÄ±mlar. `content` zorunlu string.\n",
        "- `user`: KullanÄ±cÄ± giriÅŸleri. `content` string veya (multiâ€‘modal modellerde) parÃ§a dizisi.\n",
        "- `assistant`: Ã–nceki model yanÄ±tlarÄ±. `content` string. Ä°steÄŸe baÄŸlÄ± `tool_calls` bulunabilir.\n",
        "- `tool`: Bir tool/function Ã§alÄ±ÅŸtÄ±rma sonucunu modele iletmek iÃ§in kullanÄ±lÄ±r. `tool_call_id` zorunludur.\n",
        "\n",
        "Ã–rnekler:\n",
        "\n",
        "```json\n",
        "{ \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }\n",
        "\n",
        "{ \"role\": \"user\", \"content\": \"Merhaba!\" }\n",
        "\n",
        "{\n",
        "  \"role\": \"assistant\",\n",
        "  \"content\": null,\n",
        "  \"tool_calls\": [\n",
        "    {\n",
        "      \"id\": \"call_abc123\",\n",
        "      \"type\": \"function\",\n",
        "      \"function\": {\"name\":\"get_weather\",\"arguments\":\"{\\\"city\\\":\\\"Ankara\\\"}\"}\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "{\n",
        "  \"role\": \"tool\",\n",
        "  \"tool_call_id\": \"call_abc123\",\n",
        "  \"content\": \"{\\\"temp\\\":29,\\\"unit\\\":\\\"C\\\"}\"\n",
        "}\n",
        "```\n",
        "\n",
        "#### Extra Body Parameters (vLLM Ã–zel)\n",
        "\n",
        "**vLLM Ã–zel UÃ§larÄ±n DÃ¶kÃ¼mantasyonu:** https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#chat-api_1\n",
        "\n",
        "| Field                 | Type    | Default | AÃ§Ä±klama                                  |\n",
        "| --------------------- | ------- | ------- | ----------------------------------------- |\n",
        "| `top_k`               | integer | -1      | Top-k sampling                            |\n",
        "| `min_p`               | number  | 0.0     | Minimum probability threshold             |\n",
        "| `repetition_penalty`  | number  | 1.0     | Repetition penalty                        |\n",
        "| `use_beam_search`     | boolean | false   | Beam search aktif/pasif                   |\n",
        "| `best_of`             | integer | 1       | Beam search iÃ§in kandidat sayÄ±sÄ±          |\n",
        "| `ignore_eos`          | boolean | false   | EOS token'Ä± yok say                       |\n",
        "| `min_tokens`          | integer | 0       | Minimum token sayÄ±sÄ±                      |\n",
        "| `skip_special_tokens` | boolean | true    | Ã–zel token'larÄ± atla                      |\n",
        "| `guided_json`         | object  | null    | JSON schema constraint                    |\n",
        "| `guided_regex`        | string  | null    | Regex pattern constraint                  |\n",
        "| `guided_choice`       | array   | null    | SeÃ§im kÄ±sÄ±tlamasÄ±                         |\n",
        "| `guided_grammar`      | string  | null    | Grammar constraint                        |\n",
        "| `priority`            | integer | 0       | Request priority (dÃ¼ÅŸÃ¼k = yÃ¼ksek Ã¶ncelik) |\n",
        "| `request_id`          | string  | null    | Custom request ID                         |\n",
        "\n",
        "#### Request Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"model\": \"openai/gpt-oss-20b\",\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You are a helpful assistant.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Python'da bir HTTP server nasÄ±l kurulur?\"\n",
        "    }\n",
        "  ],\n",
        "  \"max_tokens\": 150,\n",
        "  \"temperature\": 0.7,\n",
        "  \"top_p\": 0.9,\n",
        "  \"stream\": false,\n",
        "  \"extra_body\": {\n",
        "    \"top_k\": 50,\n",
        "    \"repetition_penalty\": 1.1\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Response Body (Non-streaming)\n",
        "\n",
        "| Field          | Type    | AÃ§Ä±klama                 |\n",
        "| -------------- | ------- | ------------------------ |\n",
        "| `id`           | string  | Unique response ID       |\n",
        "| `object`       | string  | \"chat.completion\"        |\n",
        "| `created`      | integer | Unix timestamp           |\n",
        "| `model`        | string  | KullanÄ±lan model adÄ±     |\n",
        "| `choices`      | array   | Response seÃ§enekleri     |\n",
        "| `usage`        | object  | Token kullanÄ±m bilgileri |\n",
        "| `service_tier` | string  | Service tier bilgisi     |\n",
        "\n",
        "#### Choices Object\n",
        "\n",
        "| Field           | Type        | AÃ§Ä±klama                                                                 |\n",
        "| --------------- | ----------- | ------------------------------------------------------------------------ |\n",
        "| `index`         | integer     | Choice index                                                             |\n",
        "| `message`       | object      | Ãœretilen assistant mesajÄ± (aÅŸaÄŸÄ±daki **Message Object (Response)**)      |\n",
        "| `finish_reason` | string/null | OlasÄ± deÄŸerler: `stop`, `length`, `tool_calls`, `content_filter`, `null` |\n",
        "| `logprobs`      | object/null | `logprobs.content` listesi dÃ¶ner (aÅŸaÄŸÄ±ya bakÄ±nÄ±z).                      |\n",
        "\n",
        "**`logprobs` formatÄ± (Chat Completions):**\n",
        "\n",
        "`logprobs.content` bir dizi nesnedir. Her eleman aÅŸaÄŸÄ±daki alanlara sahiptir:\n",
        "\n",
        "- `token` (string): Ãœretilen token.\n",
        "- `logprob` (number): Bu token'Ä±n log-olasÄ±lÄ±ÄŸÄ±.\n",
        "- `bytes` (array\\|null): Token'Ä±n UTFâ€‘8 byte karÅŸÄ±lÄ±ÄŸÄ±.\n",
        "- `top_logprobs` (array): AynÄ± konumdaki en olasÄ± alternatif tokenlarÄ±n listesi; her eleman `{ token, logprob, bytes }` iÃ§erir.\n",
        "\n",
        "#### Message Object (Response)\n",
        "\n",
        "| Field        | Type        | AÃ§Ä±klama                                                   |\n",
        "| ------------ | ----------- | ---------------------------------------------------------- |\n",
        "| `role`       | string      | Her zaman `assistant`.                                     |\n",
        "| `content`    | string/null | Ãœretilen metin iÃ§eriÄŸi. Tool Ã§aÄŸrÄ±larÄ±nda `null` olabilir. |\n",
        "| `tool_calls` | array/null  | Modelin Ã§aÄŸrÄ±lmasÄ±nÄ± istediÄŸi fonksiyonlarÄ± belirtir.      |\n",
        "\n",
        "**`tool_calls` elemanÄ±:**\n",
        "\n",
        "Her eleman aÅŸaÄŸÄ±daki ÅŸemayÄ± izler:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"call_abc123\",\n",
        "  \"type\": \"function\",\n",
        "  \"function\": {\n",
        "    \"name\": \"get_weather\",\n",
        "    \"arguments\": \"{\\\"city\\\":\\\"Ankara\\\"}\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Usage Object\n",
        "\n",
        "| Field               | Type    | AÃ§Ä±klama            |\n",
        "| ------------------- | ------- | ------------------- |\n",
        "| `prompt_tokens`     | integer | Input token sayÄ±sÄ±  |\n",
        "| `completion_tokens` | integer | Output token sayÄ±sÄ± |\n",
        "| `total_tokens`      | integer | Toplam token sayÄ±sÄ± |\n",
        "\n",
        "#### Response Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"chatcmpl-abc123\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 1692897427,\n",
        "  \"model\": \"openai/gpt-oss-20b\",\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Python'da HTTP server kurmak iÃ§in...\"\n",
        "      },\n",
        "      \"finish_reason\": \"stop\",\n",
        "      \"logprobs\": null\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 25,\n",
        "    \"completion_tokens\": 150,\n",
        "    \"total_tokens\": 175\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### 2. Embeddings API\n",
        "\n",
        "**Endpoint:** `POST /v1/embeddings`\n",
        "\n",
        "**AÃ§Ä±klama:** Text embedding'leri Ã¼retir.\n",
        "\n",
        "#### Request Body\n",
        "\n",
        "| Field             | Type         | Required | AÃ§Ä±klama                       |\n",
        "| ----------------- | ------------ | -------- | ------------------------------ |\n",
        "| `input`           | string/array | âœ“        | Embedding'i alÄ±nacak text(ler) |\n",
        "| `model`           | string       | âœ“        | Embedding model adÄ±            |\n",
        "| `encoding_format` | string       | âœ—        | \"float\" veya \"base64\"          |\n",
        "| `dimensions`      | integer      | âœ—        | Output dimension sayÄ±sÄ±        |\n",
        "\n",
        "#### Request Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"input\": [\"Hello world\", \"How are you?\"],\n",
        "  \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "  \"encoding_format\": \"float\"\n",
        "}\n",
        "```\n",
        "\n",
        "#### Response Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"object\": \"list\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"embedding\": [0.012, -0.034, 0.789, ...],\n",
        "      \"index\": 0\n",
        "    },\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"embedding\": [0.543, -0.012, 0.234, ...],\n",
        "      \"index\": 1\n",
        "    }\n",
        "  ],\n",
        "  \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 6,\n",
        "    \"total_tokens\": 6\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### 3. Models API\n",
        "\n",
        "**Endpoint:** `GET /v1/models`\n",
        "\n",
        "**AÃ§Ä±klama:** Mevcut modelleri listeler.\n",
        "\n",
        "#### Request\n",
        "\n",
        "Query parameter yok, sadece GET request.\n",
        "\n",
        "#### Response Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"object\": \"list\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"id\": \"openai/gpt-oss-20b\",\n",
        "      \"object\": \"model\",\n",
        "      \"created\": 1692897427,\n",
        "      \"owned_by\": \"vllm\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Custom vLLM Endpoints\n",
        "\n",
        "### 1. Tokenize API\n",
        "\n",
        "**Endpoint:** `POST /tokenize`\n",
        "\n",
        "**AÃ§Ä±klama:** Text'i token ID'lere Ã§evirir.\n",
        "\n",
        "#### Request Body\n",
        "\n",
        "| Field                | Type    | Required | AÃ§Ä±klama                       |\n",
        "| -------------------- | ------- | -------- | ------------------------------ |\n",
        "| `prompt`             | string  | âœ“        | Tokenize edilecek metin/prompt |\n",
        "| `add_special_tokens` | boolean | âœ—        | Ã–zel token'lar eklensin mi     |\n",
        "\n",
        "#### Request Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"prompt\": \"Hello world!\",\n",
        "  \"add_special_tokens\": true\n",
        "}\n",
        "```\n",
        "\n",
        "#### Response Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tokens\": [15496, 1917, 0],\n",
        "  \"count\": 3\n",
        "}\n",
        "```\n",
        "\n",
        "### 2. Detokenize API\n",
        "\n",
        "**Endpoint:** `POST /detokenize`\n",
        "\n",
        "**AÃ§Ä±klama:** Token ID'leri text'e Ã§evirir.\n",
        "\n",
        "#### Request Body\n",
        "\n",
        "| Field                 | Type    | Required | AÃ§Ä±klama             |\n",
        "| --------------------- | ------- | -------- | -------------------- |\n",
        "| `tokens`              | array   | âœ“        | Token ID dizisi      |\n",
        "| `skip_special_tokens` | boolean | âœ—        | Ã–zel token'larÄ± atla |\n",
        "\n",
        "#### Request Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tokens\": [15496, 1917, 0],\n",
        "  \"skip_special_tokens\": true\n",
        "}\n",
        "```\n",
        "\n",
        "#### Response Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"prompt\": \"Hello world!\"\n",
        "}\n",
        "```\n",
        "\n",
        "### 3. Classification API\n",
        "\n",
        "**Endpoint:** `POST /classify`\n",
        "\n",
        "**AÃ§Ä±klama:** Text sÄ±nÄ±flandÄ±rma iÅŸlemi.\n",
        "\n",
        "#### Request Body\n",
        "\n",
        "| Field   | Type         | Required | AÃ§Ä±klama                    |\n",
        "| ------- | ------------ | -------- | --------------------------- |\n",
        "| `model` | string       | âœ“        | Classification model adÄ±    |\n",
        "| `input` | string/array | âœ“        | SÄ±nÄ±flandÄ±rÄ±lacak text(ler) |\n",
        "\n",
        "#### Request Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"model\": \"jason9693/Qwen2.5-1.5B-apeach\",\n",
        "  \"input\": [\"I love this!\", \"This is terrible\"]\n",
        "}\n",
        "```\n",
        "\n",
        "#### Response Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"classify-7c87cac407b749a6935d8c7ce2a8fba2\",\n",
        "  \"object\": \"list\",\n",
        "  \"created\": 1745383065,\n",
        "  \"model\": \"jason9693/Qwen2.5-1.5B-apeach\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"label\": \"Default\",\n",
        "      \"probs\": [\n",
        "        0.565970778465271,\n",
        "        0.4340292513370514\n",
        "      ],\n",
        "      \"num_classes\": 2\n",
        "    },\n",
        "    {\n",
        "      \"index\": 1,\n",
        "      \"label\": \"Spoiled\",\n",
        "      \"probs\": [\n",
        "        0.26448777318000793,\n",
        "        0.7355121970176697\n",
        "      ],\n",
        "      \"num_classes\": 2\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 20,\n",
        "    \"total_tokens\": 20,\n",
        "    \"completion_tokens\": 0,\n",
        "    \"prompt_tokens_details\": null\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### 4. Score API\n",
        "\n",
        "**Endpoint:** `POST /score`\n",
        "\n",
        "**AÃ§Ä±klama:** Cross-encoder ile iki text arasÄ±nda similarity score.\n",
        "\n",
        "#### Request Body\n",
        "\n",
        "| Field    | Type   | Required | AÃ§Ä±klama                |\n",
        "| -------- | ------ | -------- | ----------------------- |\n",
        "| `model`  | string | âœ“        | Cross-encoder model adÄ± |\n",
        "| `text_1` | string | âœ“        | Ä°lk text                |\n",
        "| `text_2` | string | âœ“        | Ä°kinci text             |\n",
        "\n",
        "#### Request Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"model\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "  \"text_1\": \"What is machine learning?\",\n",
        "  \"text_2\": \"Machine learning is a subset of artificial intelligence\"\n",
        "}\n",
        "```\n",
        "\n",
        "#### Response Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"score-request-id\",\n",
        "  \"object\": \"list\",\n",
        "  \"created\": 693447,\n",
        "  \"model\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"object\": \"score\",\n",
        "      \"score\": 1\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {}\n",
        "}\n",
        "```\n",
        "\n",
        "### 5. Re-rank API\n",
        "\n",
        "**Endpoint:** `POST /v1/rerank`\n",
        "\n",
        "**AÃ§Ä±klama:** Query'ye gÃ¶re dÃ¶kÃ¼manlarÄ± yeniden sÄ±ralar.\n",
        "\n",
        "#### Request Body\n",
        "\n",
        "| Field              | Type    | Required | AÃ§Ä±klama                    |\n",
        "| ------------------ | ------- | -------- | --------------------------- |\n",
        "| `model`            | string  | âœ“        | Re-rank model adÄ±           |\n",
        "| `query`            | string  | âœ“        | Search query                |\n",
        "| `documents`        | array   | âœ“        | SÄ±ralanacak dÃ¶kÃ¼manlar      |\n",
        "| `top_n`            | integer | âœ—        | DÃ¶ndÃ¼rÃ¼lecek maksimum sonuÃ§ |\n",
        "| `return_documents` | boolean | âœ—        | DÃ¶kÃ¼man iÃ§eriÄŸini dÃ¶ndÃ¼r    |\n",
        "\n",
        "#### Request Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"model\": \"BAAI/bge-reranker-base\",\n",
        "  \"query\": \"What is the capital of France?\",\n",
        "  \"documents\": [\n",
        "    \"The capital of Brazil is Brasilia.\",\n",
        "    \"Paris is the capital and most populous city of France.\",\n",
        "    \"The Eiffel Tower is located in Paris.\"\n",
        "  ],\n",
        "  \"top_n\": 2,\n",
        "  \"return_documents\": true\n",
        "}\n",
        "```\n",
        "\n",
        "#### Response Ã–rneÄŸi\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"rerank-fae51b2b664d4ed38f5969b612edff77\",\n",
        "  \"model\": \"BAAI/bge-reranker-base\",\n",
        "  \"usage\": {\n",
        "    \"total_tokens\": 56\n",
        "  },\n",
        "  \"results\": [\n",
        "    {\n",
        "      \"index\": 1,\n",
        "      \"document\": {\n",
        "        \"text\": \"The capital of France is Paris.\"\n",
        "      },\n",
        "      \"relevance_score\": 0.99853515625\n",
        "    },\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"document\": {\n",
        "        \"text\": \"The capital of Brazil is Brasilia.\"\n",
        "      },\n",
        "      \"relevance_score\": 0.0005860328674316406\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "## KullanÄ±m Ä°puÃ§larÄ±\n",
        "\n",
        "### 1. Authentication\n",
        "\n",
        "API Key'i request header'Ä±nda gÃ¶nder:\n",
        "\n",
        "```bash\n",
        "curl -H \"Authorization: Bearer your-api-key\" \\\n",
        "     -H \"Content-Type: application/json\" \\\n",
        "     -X POST \\\n",
        "     http://localhost:8000/v1/chat/completions\n",
        "```\n",
        "\n",
        "### 2. Streaming Responses\n",
        "\n",
        "Streaming iÃ§in `Accept` header'Ä±nÄ± ekle:\n",
        "\n",
        "```bash\n",
        "curl -H \"Accept: text/event-stream\" \\\n",
        "     -H \"Authorization: Bearer your-api-key\" \\\n",
        "     -H \"Content-Type: application/json\" \\\n",
        "     -d '{\"stream\": true, ...}' \\\n",
        "     http://localhost:8000/v1/chat/completions\n",
        "```\n",
        "\n",
        "### 3. Function Calling\n",
        "\n",
        "Tool tanÄ±mÄ± ve kullanÄ±mÄ±:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"tools\": [\n",
        "    {\n",
        "      \"type\": \"function\",\n",
        "      \"function\": {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Hava durumu bilgisi al\",\n",
        "        \"parameters\": {\n",
        "          \"type\": \"object\",\n",
        "          \"properties\": {\n",
        "            \"city\": { \"type\": \"string\", \"description\": \"Åžehir adÄ±\" }\n",
        "          },\n",
        "          \"required\": [\"city\"]\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  \"tool_choice\": \"auto\"\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "bgel6mMCgeZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vLLM InstancelarÄ±nÄ± NasÄ±l KapatÄ±rÄ±m?"
      ],
      "metadata": {
        "id": "2qMGSFSjo3vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J4tjuuZqonH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Birden fazla **vLLM instance** aynÄ± anda ayaklanmÄ±ÅŸsa (Ã¶zellikle aralarÄ±nda `generator` tipi process varsa), bunlar Ã§akÄ±ÅŸmalara sebep olabilir. Bu durumda ilgili process IDâ€™lerini (PID) bulup kapatmak gerekir.\n",
        "\n",
        "### Ã‡alÄ±ÅŸan vLLM processâ€™lerini listelemek\n",
        "```bash\n",
        "komut1: ps aux | grep vllm\n",
        "komut1_alternatif: lsof -i:8000\n",
        "\n",
        "Ã¶rnek Ã§Ä±ktÄ±: user     12345  2.3  5.6 12345678 456789 ? Sl   12:00   0:05 python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B-Instruct\n",
        "\n",
        "komut2: kill -9 12345\n",
        "```\n",
        "\n",
        "Bu komutlardan sonra sorun Ã§Ã¶zÃ¼lmezse Ã§alÄ±ÅŸma zamanÄ±nÄ± tekrar baÅŸlatÄ±n."
      ],
      "metadata": {
        "id": "yxE3MALX_qla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!lsof -i:8000"
      ],
      "metadata": {
        "id": "iC4Cu2eW3CRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ps aux | grep vllm"
      ],
      "metadata": {
        "id": "yFkstv7V3HIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e771ce92-8afc-4f25-92df-db8bd16521e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        3736  8.1  2.8 12608884 1577208 ?    Sl   14:10   0:19 python3 -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B-Instruct --task generate --host 0.0.0.0 --port 8000 --dtype float16 --max-num-seqs 8 --enable-auto-tool-choice --tool-call-parser hermes\n",
            "root        4939  0.0  0.0   7376  3440 ?        S    14:14   0:00 /bin/bash -c ps aux | grep vllm\n",
            "root        5015  0.0  0.0   6484  2320 ?        S    14:14   0:00 grep vllm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!kill -9 123456"
      ],
      "metadata": {
        "id": "EOmYBIt-_wA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qwen/Qwen2.5-7B-Instruct LLM Modeli"
      ],
      "metadata": {
        "id": "sAOfXvmNqDK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model Qwen/Qwen2.5-7B-Instruct \\\n",
        "  --task generate \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port 8000 \\\n",
        "  --dtype float16 --gpu-memory-utilization 0.9 --max-num-seqs 8 \\\n",
        "  --enable-auto-tool-choice \\\n",
        "  --tool-call-parser hermes \\\n",
        "  > generator.log 2>&1 &\n"
      ],
      "metadata": {
        "id": "OqzlmsLnXcZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:8000/v1\",\n",
        "    api_key=\"dummy\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "tyK4dZ20qM8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelleri Listele â€” GET /v1/models\n",
        "client.models.list() Ã§aÄŸrÄ±sÄ± bu uÃ§ noktayÄ± kullanÄ±r."
      ],
      "metadata": {
        "id": "voUfnk5UCn-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = client.models.list()\n",
        "\n",
        "for m in models.data:\n",
        "    print(m.id)\n",
        "    MODEL = m.id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm0RLmGLqKo_",
        "outputId": "cd07f3d4-4ca0-4b83-85e1-93130f9387b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen/Qwen2.5-7B-Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat â€” POST /v1/chat/completions Vanilla"
      ],
      "metadata": {
        "id": "MDbKQu3FC2BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Bu API ne iÅŸe yarar? Bir cÃ¼mlede Ã¶zetle.\"},\n",
        "]\n",
        "\n",
        "try:\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        max_completion_tokens=500,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    choice = resp.choices[0]\n",
        "    print(\"finish_reason:\", choice.finish_reason)\n",
        "    if resp.usage:\n",
        "        print(\"usage:\", resp.usage.model_dump())\n",
        "    print(\"\\nAssistant:\")\n",
        "    print(choice.message.content)\n",
        "except Exception as e:\n",
        "    print(\"Chat completions (temel) hatasÄ±:\", e)\n"
      ],
      "metadata": {
        "id": "DnNvQGWB3Ifn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff247941-dbdf-4793-daec-aa153ea03557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish_reason: stop\n",
            "usage: {'completion_tokens': 33, 'prompt_tokens': 35, 'total_tokens': 68, 'completion_tokens_details': None, 'prompt_tokens_details': None}\n",
            "\n",
            "Assistant:\n",
            "Bu API, belirli bir fonksiyon veya hizmeti otomatikleÅŸtirerek veya programlÄ± olarak eriÅŸmek iÃ§in kullanÄ±lÄ±r.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat â€” POST /v1/chat/completions Streaming"
      ],
      "metadata": {
        "id": "xjG-oRrVD0Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"chat/completions ile bir streaming Ã¶rneÄŸini gÃ¶sterir misin? 2-3 cÃ¼mlelik bir yanÄ±t Ã¼ret.\"},\n",
        "]\n",
        "\n",
        "full_text = []\n",
        "try:\n",
        "    stream = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        stream=True,\n",
        "        temperature=0.3,\n",
        "        max_tokens=200,\n",
        "    )\n",
        "\n",
        "    for event in stream:\n",
        "        if getattr(event, \"choices\", None):\n",
        "            delta = event.choices[0].delta\n",
        "            content = getattr(delta, \"content\", None)\n",
        "            if content:\n",
        "                full_text.append(content)\n",
        "                sys.stdout.write(content)\n",
        "                sys.stdout.flush()\n",
        "            if getattr(event.choices[0], \"finish_reason\", None):\n",
        "                print(\"\\n[finish_reason]\", event.choices[0].finish_reason)\n",
        "\n",
        "    # Tam metni sonradan kullanmak istersen:\n",
        "    final_text = \"\".join(full_text)\n",
        "except Exception as e:\n",
        "    print(\"Streaming chat hatasÄ±:\", e)"
      ],
      "metadata": {
        "id": "q2axcKG0C5lN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cdccd4-36a3-427e-b01b-72226473fc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabii, bir streaming Ã¶rneÄŸi olarak: \"BaÅŸlÄ±yoruz... Yerel zamanlÄ± hava durumu gÃ¼ncellemelerini alabilmek iÃ§in chat/completions API'sini streaming moduyla Ã§aÄŸÄ±rabiliriz.\"\n",
            "[finish_reason] stop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat â€” POST /v1/chat/completions (response_format)"
      ],
      "metadata": {
        "id": "IaqhoQu-D4Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a data API that returns JSON only. Always return a single valid JSON object with the requested fields and nothing else.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Åžu metinden {title, keywords[]} alanlarÄ±nÄ± JSON olarak Ã§Ä±kar ve sadece geÃ§erli bir JSON nesnesi dÃ¶ndÃ¼r: 'Agentic AI ile RAG mimarisini birleÅŸtiren bir demo hazÄ±rlayacaÄŸÄ±z.'\"},\n",
        "]\n",
        "\n",
        "try:\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0.0,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "    print(resp.choices[0].message.content)  # GeÃ§erli JSON string\n",
        "except Exception as e:\n",
        "    print(\"Structured JSON hatasÄ±:\", e)\n"
      ],
      "metadata": {
        "id": "9yo_5MlGD5wr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67835a2-5940-4456-9edf-f999c21da7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"title\": \"Agentic AI ile RAG mimarisini birleÅŸtiren bir demo\",\n",
            "  \"keywords\": [\"Agentic AI\", \"RAG mimarisi\", \"demo\"]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a linguist who teaches Turkish. Return JSON only. Always return a single valid JSON object with the requested arrays.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Åžu metinden {erkek[], kadÄ±n[]} alanlarÄ±nÄ± JSON olarak Ã§Ä±kar ve sadece geÃ§erli bir JSON nesnesi dÃ¶ndÃ¼r: 'AyÅŸe, Fatma, Esra, Batuhan, Mehmet, Ali'\"},\n",
        "]\n",
        "\n",
        "try:\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0.0,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "    print(resp.choices[0].message.content)  # GeÃ§erli JSON string\n",
        "except Exception as e:\n",
        "    print(\"Structured JSON hatasÄ±:\", e)\n"
      ],
      "metadata": {
        "id": "-ai3QbknD_jV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6d75e1-9655-41f0-ba03-28235592cf62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"erkek\":[\"Batuhan\", \"Mehmet\", \"Ali\"], \"kadÄ±n\":[\"AyÅŸe\", \"Fatma\", \"Esra\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat â€” POST /v1/chat/completions Function/Tool Calling"
      ],
      "metadata": {
        "id": "xs9shOXPE6u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "# 1) Statik deÄŸer dÃ¶nen fonksiyonu tanÄ±mla\n",
        "\n",
        "def get_weather(city: str, date: Optional[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Statik hava durumu dÃ¶ndÃ¼rÃ¼r (Ã¶rnek amaÃ§lÄ±).\"\"\"\n",
        "    return {\n",
        "        \"city\": city,\n",
        "        \"date\": date or \"2025-08-18\",\n",
        "        \"tempC\": 45,\n",
        "        \"desc\": \"GÃ¼neÅŸli\",\n",
        "    }\n",
        "\n",
        "# 2) Fonksiyon adÄ±ndan gerÃ§ek Python fonksiyonuna giden mapping\n",
        "function_registry = {\n",
        "    \"get_weather\": get_weather,\n",
        "}\n",
        "\n",
        "# 3) Tool ÅŸemasÄ±nÄ± ve konuÅŸmayÄ± yeniden tanÄ±mla (self-contained olsun)\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Returns weather based on given city and date informations. Date is not neccessary.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"city\": {\"type\": \"string\"},\n",
        "                    \"date\": {\"type\": \"string\", \"description\": \"YYYY-MM-DD\"},\n",
        "                },\n",
        "                \"required\": [\"city\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a weather assistant. Use tool_call if you think that is needed.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Ankara iÃ§in bugÃ¼n hava nasÄ±l?\"},\n",
        "]\n",
        "\n",
        "# 4) Ä°lk Ã§aÄŸrÄ±: model gerek gÃ¶rÃ¼rse tool_call isteÄŸi yollar\n",
        "try:\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",\n",
        "        temperature=0,\n",
        "        #store=True,\n",
        "    )\n",
        "    msg = resp.choices[0].message\n",
        "    print(\"finish_reason:\", resp.choices[0].finish_reason)\n",
        "\n",
        "    if not getattr(msg, \"tool_calls\", None):\n",
        "        print(\"Tool Ã§aÄŸrÄ±sÄ± yok, yanÄ±t:\")\n",
        "        print(msg.content)\n",
        "    else:\n",
        "        # 5) Tool Ã§aÄŸrÄ±larÄ±nÄ± Ã§alÄ±ÅŸtÄ±r ve sonucu modele geri ver\n",
        "        for tc in msg.tool_calls:\n",
        "            tool_name = tc.function.name\n",
        "            raw_args = tc.function.arguments or \"{}\"\n",
        "            try:\n",
        "                args = json.loads(raw_args)\n",
        "            except Exception:\n",
        "                # BazÄ± sunucular JSON string yerine dÃ¼z metin verebilir\n",
        "                # Basit bir fallback: city'i ham metinden Ã§Ä±karamÄ±yorsak boÅŸ geÃ§elim\n",
        "                args = {}\n",
        "\n",
        "            fn = function_registry.get(tool_name)\n",
        "            if not fn:\n",
        "                print(f\"TanÄ±msÄ±z tool: {tool_name}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                result = fn(**args)\n",
        "            except TypeError:\n",
        "                # Eksik/extra argÃ¼man varsa sadece beklenenleri geÃ§ir\n",
        "                safe = {}\n",
        "                for p in (\"city\", \"date\"):\n",
        "                    if p in args:\n",
        "                        safe[p] = args[p]\n",
        "                result = fn(**safe)\n",
        "\n",
        "            # 6) Tool sonucunu modele geri gÃ¶nder\n",
        "            followup = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    *messages,\n",
        "                    msg,  # tool Ã§aÄŸrÄ±sÄ±nÄ± iÃ§eren assistant mesajÄ±\n",
        "                    {\n",
        "                        \"role\": \"tool\",\n",
        "                        \"tool_call_id\": tc.id,\n",
        "                        \"content\": json.dumps(result, ensure_ascii=False),\n",
        "                    },\n",
        "                ],\n",
        "                temperature=0,\n",
        "            )\n",
        "            print(\"\\nFinal assistant:\")\n",
        "            print(followup.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(\"Tool calling (mapped) hatasÄ±:\", e)"
      ],
      "metadata": {
        "id": "g6tTbZw5E9ui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4840aab7-e798-4b70-fc28-1a700f73ceea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish_reason: tool_calls\n",
            "\n",
            "Final assistant:\n",
            "Ankara'da bugÃ¼n hava Ã§ok sÄ±cak ve gÃ¼neÅŸli. SÄ±caklÄ±k 45Â°C'e ulaÅŸacak. DÄ±ÅŸarÄ±da uzun sÃ¼re kalmanÄ±zÄ± Ã¶nermeyebiliriz.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## /tokenize"
      ],
      "metadata": {
        "id": "xiWwa2Fj-pZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "URL = \"http://localhost:8000/tokenize\"\n",
        "\n",
        "payload = {\n",
        "    \"prompt\": \"Merhaba dÃ¼nya! Bu bir tokenizasyon testidir.\"\n",
        "}\n",
        "\n",
        "res = requests.post(URL, json=payload, timeout=30)\n",
        "res.raise_for_status()\n",
        "\n",
        "tok = res.json()\n",
        "tok['tokens']\n",
        "print(\"ðŸ§© Tokenize sonucu:\", tok['tokens'])\n",
        "\n"
      ],
      "metadata": {
        "id": "o4ciX5_ixTUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0dee1ec-0eb3-4124-fce4-bbcd8a0c982a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§© Tokenize sonucu: [26716, 10573, 64, 129463, 0, 27227, 15248, 3950, 449, 6405, 263, 1273, 307, 404, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## /detokenize"
      ],
      "metadata": {
        "id": "1LYWdgOE5j9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"http://localhost:8000/detokenize\"  # vLLM Detokenizer API\n",
        "# input_ids: tokenize Ã§aÄŸrÄ±sÄ±ndan aldÄ±ÄŸÄ±n ID'ler\n",
        "payload = {\n",
        "    \"tokens\": [26716, 10573, 64, 129463, 0, 27227, 15248, 3950, 449, 6405, 263, 1273, 307, 404, 13]\n",
        "    }\n",
        "res = requests.post(URL, json=payload, timeout=30)\n",
        "res.raise_for_status()\n",
        "\n",
        "detok = res.json()\n",
        "print(\"ðŸ§µ Detokenize sonucu:\", detok)"
      ],
      "metadata": {
        "id": "LwQT_ydFxhCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d779068-9787-4def-976c-024034643b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§µ Detokenize sonucu: {'prompt': 'Merhaba dÃ¼nya! Bu bir tokenizasyon testidir.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# /v1/embeddings Embedding Modeli"
      ],
      "metadata": {
        "id": "vPG4VGLMpvV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model BAAI/bge-small-en-v1.5 \\\n",
        "  --task embed \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port 8001 > embedding.log 2>&1 &\n"
      ],
      "metadata": {
        "id": "q1Ker_xGm1FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "resp = requests.post(\n",
        "    \"http://localhost:8001/v1/embeddings\",\n",
        "    headers={\"Content-Type\": \"application/json\"},\n",
        "    json={\"input\": \"Hello World\", \"model\": \"BAAI/bge-small-en-v1.5\"}\n",
        ")\n",
        "\n",
        "print(\"Vectoral representation of the input sentence:\" ,resp.json()['data'][0]['embedding'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfy72bqKm7O6",
        "outputId": "5d2c7d0c-e45f-4053-dded-2b2e5b8d593b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectoral representation of the input sentence: [0.0151519775390625, -0.02252197265625, 0.00853729248046875, -0.07421875, 0.0038585662841796875, 0.0027217864990234375, -0.0312347412109375, 0.04461669921875, 0.04400634765625, -0.00785064697265625, -0.0252532958984375, -0.033355712890625, 0.0143890380859375, 0.046539306640625, 0.0085601806640625, -0.01617431640625, 0.007381439208984375, -0.01898193359375, -0.11480712890625, -0.0181884765625, 0.1263427734375, 0.0297088623046875, 0.025299072265625, -0.034210205078125, -0.04107666015625, 0.006603240966796875, 0.01036834716796875, 0.022430419921875, 0.004444122314453125, -0.1273193359375, -0.0161590576171875, -0.020416259765625, 0.0472412109375, 0.0115966796875, 0.068115234375, 0.007297515869140625, -0.017852783203125, 0.040771484375, -0.01030731201171875, 0.0237274169921875, 0.0106048583984375, -0.028594970703125, 0.00811767578125, -0.015167236328125, 0.0309600830078125, -0.06597900390625, -0.0220947265625, 0.05401611328125, 0.002521514892578125, 0.022430419921875, -0.09161376953125, -0.04510498046875, -0.00420379638671875, -0.005641937255859375, -0.005435943603515625, 0.098388671875, 0.060546875, 0.0073699951171875, 0.0139312744140625, 0.0027103424072265625, 0.047637939453125, 0.0286407470703125, -0.1552734375, 0.0689697265625, 0.0302276611328125, -0.017974853515625, 0.0210113525390625, 0.021392822265625, 0.0141448974609375, 0.0018329620361328125, 0.002696990966796875, 0.00390625, 0.0411376953125, 0.06591796875, -0.0061492919921875, -0.01654052734375, 0.0081634521484375, -0.048980712890625, -0.02105712890625, -0.0308685302734375, -0.04052734375, 0.0592041015625, 0.0182037353515625, -0.044281005859375, 0.00067901611328125, -0.027862548828125, -0.040679931640625, -0.01126861572265625, -0.0249786376953125, 0.00966644287109375, -0.0175323486328125, -0.0272216796875, -0.015350341796875, -0.00536346435546875, -0.0413818359375, 0.00717926025390625, 0.0070953369140625, 0.009765625, 0.0005350112915039062, 0.34423828125, -0.0953369140625, -0.00196075439453125, 0.028106689453125, -0.09130859375, 0.0595703125, 0.0248870849609375, -0.01641845703125, -0.02911376953125, -0.008331298828125, 0.015716552734375, 0.01282501220703125, -0.06427001953125, 0.01450347900390625, -0.01372528076171875, 0.00104522705078125, -0.0196380615234375, 0.050048828125, -0.0027942657470703125, 0.09320068359375, -0.0294952392578125, -0.008056640625, 0.0307159423828125, -0.043975830078125, -0.00421142578125, 0.05291748046875, -0.064453125, 0.058135986328125, 0.07757568359375, 0.01160430908203125, 0.06982421875, -0.00548553466796875, 0.059844970703125, -0.026336669921875, -0.00867462158203125, 0.02752685546875, -0.0142974853515625, -0.018218994140625, -0.013946533203125, 0.035552978515625, -0.0567626953125, 0.0081939697265625, -0.07666015625, -0.0225982666015625, -0.11279296875, 0.00030922889709472656, 0.030303955078125, -0.0733642578125, 0.024566650390625, -0.0196533203125, -0.0240325927734375, -0.038970947265625, 0.0787353515625, 0.004978179931640625, -0.01629638671875, 0.00774383544921875, 0.05511474609375, -0.01274871826171875, 0.068359375, 0.0077972412109375, 0.00876617431640625, -0.0018177032470703125, -0.012420654296875, -0.01328277587890625, 0.0066680908203125, -0.01776123046875, -0.1280517578125, 0.0099945068359375, 0.0194091796875, -0.007213592529296875, 0.0007925033569335938, 0.0032558441162109375, 0.016571044921875, -0.03961181640625, 0.0288543701171875, 0.1094970703125, 0.007511138916015625, -0.00403594970703125, 0.044586181640625, -0.047210693359375, 0.0251007080078125, 0.060089111328125, -0.0509033203125, -0.04168701171875, 0.01904296875, 0.028289794921875, -0.02532958984375, -0.0207977294921875, -0.030487060546875, 0.062225341796875, 0.06707763671875, -0.0231170654296875, 0.0106048583984375, -0.0318603515625, -0.034271240234375, -0.084228515625, 0.0033855438232421875, 0.033966064453125, -0.0810546875, 0.0134429931640625, -0.0215301513671875, 0.1463623046875, 0.053070068359375, 0.0040740966796875, 0.028778076171875, 0.0005903244018554688, 0.004177093505859375, 0.040679931640625, 0.00618743896484375, 0.04486083984375, 0.01337432861328125, -0.0242767333984375, -0.015167236328125, 0.07318115234375, -0.00650787353515625, 0.0218963623046875, -0.04290771484375, -0.0099945068359375, 0.07464599609375, 0.0238494873046875, 0.047119140625, -0.039794921875, 0.01079559326171875, -0.02215576171875, -0.262451171875, 0.01800537109375, 0.00820159912109375, -0.0033855438232421875, -0.0347900390625, 0.0229644775390625, 0.038055419921875, -0.05157470703125, 0.101806640625, -0.00907135009765625, 0.0870361328125, -0.05963134765625, -0.00824737548828125, -0.03643798828125, 0.017547607421875, 0.023284912109375, -0.01409149169921875, 0.0159759521484375, -0.01007080078125, -0.0226898193359375, 0.028564453125, 0.02288818359375, 0.043365478515625, -0.047637939453125, 0.04443359375, -0.0595703125, 0.1466064453125, 0.08367919921875, -0.0202178955078125, 0.024261474609375, 0.03631591796875, -0.0279998779296875, -0.009307861328125, -0.1197509765625, -0.0255889892578125, 0.0736083984375, -0.03460693359375, -0.0672607421875, -0.09661865234375, -0.022308349609375, -0.0124359130859375, 0.0137786865234375, -0.0408935546875, -0.0043182373046875, -0.024139404296875, -0.0748291015625, -0.052581787109375, 0.009765625, -0.052032470703125, -0.01244354248046875, -0.01174163818359375, 0.0223846435546875, 0.05712890625, 0.060028076171875, 0.019073486328125, -0.045928955078125, 0.0016622543334960938, -0.0007357597351074219, -0.01146697998046875, 0.0323486328125, -0.0146636962890625, -0.0222015380859375, 0.01580810546875, -0.036651611328125, 0.01152801513671875, 0.035064697265625, -0.061065673828125, -0.024932861328125, 0.049835205078125, -0.0174407958984375, -0.01812744140625, -0.035736083984375, 0.0212554931640625, -0.0164337158203125, 0.0362548828125, 0.014190673828125, -0.004505157470703125, -0.0233001708984375, -0.039825439453125, -0.0281524658203125, -0.005496978759765625, 0.01141357421875, 0.05828857421875, 0.01427459716796875, 0.032745361328125, 0.05401611328125, 0.06463623046875, 0.00775909423828125, 0.035400390625, -0.0160675048828125, -0.012939453125, 0.04119873046875, -0.005306243896484375, -0.06982421875, 0.0113067626953125, 0.016082763671875, -0.295166015625, 0.02777099609375, -0.00298309326171875, 0.021392822265625, 0.004032135009765625, 0.0210723876953125, 0.0411376953125, -0.0004038810729980469, -0.057342529296875, 0.0222625732421875, -0.07745361328125, 0.0203704833984375, 0.016204833984375, -0.06689453125, 0.0008029937744140625, 0.020263671875, -0.002460479736328125, -0.0110015869140625, 0.0170135498046875, -0.0195770263671875, 0.00208282470703125, 0.0220794677734375, 0.229736328125, -0.023040771484375, 0.0567626953125, 0.039093017578125, -0.009246826171875, 0.00460052490234375, 0.0548095703125, 0.019317626953125, -0.09814453125, -0.0001900196075439453, 0.031524658203125, -0.015625, 0.035430908203125, 0.01094818115234375, -0.0679931640625, -0.0289306640625, 0.0240478515625, -0.053131103515625, -0.0249786376953125, 0.0223541259765625, -0.045989990234375, 0.07037353515625, 0.034576416015625, -0.07733154296875, -0.01348114013671875, -0.048980712890625, -0.00394439697265625, 0.037353515625, -0.0281982421875, -0.0797119140625, 0.005672454833984375, 0.032073974609375, -0.030517578125, 0.0150604248046875, 0.01476287841796875, -0.009063720703125, 0.0160980224609375, -0.06341552734375, 0.0213165283203125, -0.00611114501953125, 0.04925537109375, 0.0228118896484375, 0.0260467529296875]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# /v1/rerank RE-RANK Modeli"
      ],
      "metadata": {
        "id": "5FUM6x50pmTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model cross-encoder/ms-marco-MiniLM-L-6-v2 \\\n",
        "  --task score \\\n",
        "  --host 0.0.0.0 \\\n",
        "  --port 8002 \\\n",
        "  > reranker.log 2>&1 &"
      ],
      "metadata": {
        "id": "xdfiXz2fpgMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## /v1/rerank"
      ],
      "metadata": {
        "id": "hiIx_32aQ717"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "URL = \"http://localhost:8002/v1/rerank\"\n",
        "api_key = \"dummy\"\n",
        "\n",
        "payload = {\n",
        "    \"model\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    \"query\": \"Hassas ciltler iÃ§in en uygun doÄŸal bakÄ±m Ã¼rÃ¼nÃ¼ hangisi?\",\n",
        "    \"top_n\": 2,\n",
        "    \"documents\": [\n",
        "        \"Aloe vera ve papatya iÃ§eren doÄŸal cilt bakÄ±m kremi, hassas ciltler iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸtÄ±r. TahriÅŸi azaltÄ±r ve nem dengesini korur.\",\n",
        "        \"Bu yÄ±lÄ±n makyaj trendleri cesur renkler ve parlak dokular Ã¼zerine yoÄŸunlaÅŸÄ±yor. Neon eyeliner ve Ä±ÅŸÄ±ltÄ±lÄ± highlighter Ã¶ne Ã§Ä±kÄ±yor.\",\n",
        "        \"Shea yaÄŸÄ± ve jojoba yaÄŸÄ± ile Ã¼retilmiÅŸ organik nemlendirici, hassas ciltleri yatÄ±ÅŸtÄ±rÄ±r ve uzun sÃ¼reli koruma saÄŸlar.\"\n",
        "    ],\n",
        "    \"return_documents\": True\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {api_key}\"\n",
        "}\n",
        "\n",
        "response = requests.post(URL, headers=headers, json=payload)\n",
        "\n",
        "if response.ok:\n",
        "    data = response.json()\n",
        "    results = data.get(\"results\", [])\n",
        "\n",
        "    # relevance_score'a gÃ¶re sÄ±rala (yÃ¼ksekten dÃ¼ÅŸÃ¼ÄŸe)\n",
        "    results_sorted = sorted(results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
        "\n",
        "    print(\"\\nðŸ“Œ Re-rank SÄ±ralamasÄ±:\")\n",
        "    for i, item in enumerate(results_sorted, start=1):\n",
        "        score = item[\"relevance_score\"]\n",
        "        doc_text = item[\"document\"][\"text\"]\n",
        "        print(f\"{i}. (Skor: {score:.4f}) â†’ {doc_text}\")\n",
        "else:\n",
        "    print(\"Hata:\", response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnFw3yxkxOMb",
        "outputId": "751bb6a8-d7a7-47d4-c6ea-373b1c0b1402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Œ Re-rank SÄ±ralamasÄ±:\n",
            "1. (Skor: 4.6328) â†’ Aloe vera ve papatya iÃ§eren doÄŸal cilt bakÄ±m kremi, hassas ciltler iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸtÄ±r. TahriÅŸi azaltÄ±r ve nem dengesini korur.\n",
            "2. (Skor: 0.1362) â†’ Shea yaÄŸÄ± ve jojoba yaÄŸÄ± ile Ã¼retilmiÅŸ organik nemlendirici, hassas ciltleri yatÄ±ÅŸtÄ±rÄ±r ve uzun sÃ¼reli koruma saÄŸlar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## /v1/score"
      ],
      "metadata": {
        "id": "1kvUp1Uk1v2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://127.0.0.1:8002/v1/score\"\n",
        "\n",
        "payload = {\n",
        "    \"model\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    \"encoding_format\": \"float\",\n",
        "    \"text_1\": \"Hassas ciltler iÃ§in en uygun doÄŸal bakÄ±m Ã¼rÃ¼nÃ¼ hangisi?\",\n",
        "    \"text_2\": \"Aloe vera ve papatya iÃ§eren doÄŸal cilt bakÄ±m kremi, hassas ciltler iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸtÄ±r. TahriÅŸi azaltÄ±r ve nem dengesini korur.\",\n",
        "}\n",
        "\n",
        "headers = {\"Content-Type\": \"application/json\", \"accept\": \"application/json\"}\n",
        "\n",
        "res = requests.post(url, json=payload, headers=headers, timeout=30)\n",
        "data = res.json()\n",
        "score = data[\"data\"][0][\"score\"]\n",
        "text = payload[\"text_2\"]  # karÅŸÄ±laÅŸtÄ±rÄ±lan cÃ¼mle\n",
        "print(f'(Skor: {score:.4f}) -> {text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlJO7fQkzhOs",
        "outputId": "0bb677ce-c4b2-43a8-a52e-845db58f829e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Skor: 4.6328) -> Aloe vera ve papatya iÃ§eren doÄŸal cilt bakÄ±m kremi, hassas ciltler iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸtÄ±r. TahriÅŸi azaltÄ±r ve nem dengesini korur.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# /classify Classification Modeli"
      ],
      "metadata": {
        "id": "d6Vj81771kk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model unitary/multilingual-toxic-xlm-roberta \\\n",
        "  --task classify \\\n",
        "  --port 8003 \\\n",
        "  > classifier.log 2>&1 &"
      ],
      "metadata": {
        "id": "DT_HvfNF1siu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from transformers import AutoConfig\n",
        "\n",
        "url = \"http://localhost:8003/classify\"\n",
        "MODEL = \"unitary/multilingual-toxic-xlm-roberta\"\n",
        "\n",
        "inputs = [\n",
        "    \"I am going to find you, and I am going to kill you\",\n",
        "    \"What the hell is your problem man?\",\n",
        "    \"I love you so much\"\n",
        "]\n",
        "\n",
        "# 1) id2label (etiket isimleri) al\n",
        "try:\n",
        "    cfg = AutoConfig.from_pretrained(MODEL)\n",
        "    id2label = {int(k): v for k, v in getattr(cfg, \"id2label\", {}).items()}\n",
        "except Exception:\n",
        "    id2label = {}\n",
        "\n",
        "# 2) vLLM'e isteÄŸi at\n",
        "payload = {\"model\": MODEL, \"task\": \"classify\", \"input\": inputs}\n",
        "resp = requests.post(url, json=payload, timeout=30)\n",
        "resp.raise_for_status()\n",
        "out = resp.json()\n",
        "\n",
        "# 3) SonuÃ§larÄ± yazdÄ±r\n",
        "for item in out.get(\"data\", []):\n",
        "    idx = item[\"index\"]\n",
        "    probs = item[\"probs\"]   # burada sadece 1 deÄŸer var: toxic ihtimali\n",
        "    toxic_prob = probs[0]\n",
        "\n",
        "    print(f\"\\nðŸ“Œ Input[{idx}]: \\\"{inputs[idx]}\\\"\")\n",
        "    print(f\"Probability of being TOXIC = {toxic_prob:.4f} ({toxic_prob*100:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sFF3hhEGhx-",
        "outputId": "5642f3e6-f9a9-4883-c332-b311dfff86d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Œ Input[0]: \"I am going to find you, and I am going to kill you\"\n",
            "Probability of being TOXIC = 0.9922 (99.2%)\n",
            "\n",
            "ðŸ“Œ Input[1]: \"What the hell is your problem man?\"\n",
            "Probability of being TOXIC = 0.7744 (77.4%)\n",
            "\n",
            "ðŸ“Œ Input[2]: \"I love you so much\"\n",
            "Probability of being TOXIC = 0.0005 (0.0%)\n"
          ]
        }
      ]
    }
  ]
}